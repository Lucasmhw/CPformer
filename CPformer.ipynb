{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17459768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================  CONFIG  ======================\n",
    "CFG = dict(\n",
    "    csv_path   = r\"\",  \n",
    "    date_col   = \"date\",      \n",
    "    lookback   = 120,\n",
    "    tau        = 25,\n",
    "    pred_len   = 96,\n",
    "    d_model    = 64,\n",
    "    nhead      = 4,\n",
    "    num_layers = 2,\n",
    "    dropout    = 0.001,\n",
    "    concept_hidden = [64, 32],\n",
    "    mlp_hidden     = [128, 64],\n",
    "    lr         = 0.0002,\n",
    "    epochs     = 10,\n",
    "    batch_size = 64,\n",
    "    lamb_phys  = 1,\n",
    "    lamb_con   = 1,  \n",
    "    lambs      = (1, 1, 1, 1, 1),\n",
    "    device     = \"cuda\" if __import__(\"torch\").cuda.is_available() else \"cpu\",\n",
    "    plot_each  = True,\n",
    "    show_epoch_mse = True,\n",
    "    scale_method = \"standard\",    \n",
    ")\n",
    "# =======================================================\n",
    "import warnings, numpy as np, pandas as pd, torch, \\\n",
    "       torch.nn as nn, torch.nn.functional as F, matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "\n",
    "def load_csv(path, date_col):\n",
    "    df = pd.read_csv(path)\n",
    "    if date_col and date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        df = df.set_index(date_col).sort_index()\n",
    "    return df\n",
    "\n",
    "def window_stack(arr, L):\n",
    "    \"\"\"Rolling view of length‑L windows as ndarray (n_win, L).\"\"\"\n",
    "    return np.stack([arr[i : i + L] for i in range(len(arr) - L + 1)], axis=0)\n",
    "\n",
    "def ensure_list(x):\n",
    "    return x if isinstance(x, (list, tuple)) else [x]\n",
    "\n",
    "def build_mlp(in_dim, hidden, out_dim):\n",
    "    layers, last = [], in_dim\n",
    "    for h in hidden:\n",
    "        layers += [nn.Linear(last, h), nn.ReLU(inplace=True)]\n",
    "        last = h\n",
    "    layers.append(nn.Linear(last, out_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def get_scaler():\n",
    "    return MinMaxScaler() if CFG[\"scale_method\"].lower() == \"minmax\" else StandardScaler()\n",
    "\n",
    "\n",
    "class SeriesDataset(Dataset):\n",
    "    def __init__(self, arr, L, tau):\n",
    "        self.win = torch.from_numpy(window_stack(arr, L)).float()\n",
    "        self.tau = tau\n",
    "    def __len__(self):\n",
    "        return len(self.win) - self.tau\n",
    "    def __getitem__(self, idx):\n",
    "        w = self.win[idx]\n",
    "        y = self.win[idx + self.tau, -1]\n",
    "        return w, y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, L, d_model, nhead, nlayers, dropout):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, L, d_model))\n",
    "        self.in_proj = nn.Linear(1, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, nlayers)\n",
    "    def forward(self, x):\n",
    "        x = self.in_proj(x.unsqueeze(-1)) + self.pos_emb\n",
    "        return self.transformer(x)\n",
    "\n",
    "class ConceptLayer(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.mlp = build_mlp(d_model, ensure_list(CFG[\"concept_hidden\"]), 5)\n",
    "    def forward(self, z):\n",
    "        return self.mlp(z[:, -1])\n",
    "\n",
    "class PINNHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = build_mlp(5, ensure_list(CFG[\"mlp_hidden\"]), 1)\n",
    "    def forward(self, c):\n",
    "        return self.fc(c).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "def physics_residual(last_x, C, head):\n",
    "    preds = head(C)\n",
    "    r1 = torch.mean((preds - last_x) ** 2)\n",
    "    r2 = torch.mean(C ** 2)\n",
    "    a, b = CFG[\"lambs\"][:2]\n",
    "    return CFG[\"lamb_phys\"] * (a * r1 + b * r2)\n",
    "\n",
    "\n",
    "\n",
    "def hard_concepts(win, tau):\n",
    "    x = win[:, -tau:].mean(dim=1)\n",
    "    return torch.stack([\n",
    "        (x > 0).float(),\n",
    "        (x < 0).float(),\n",
    "        torch.abs(x),\n",
    "        torch.exp(-x),\n",
    "        torch.sin(x),\n",
    "    ], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "def train_series(arr_std):\n",
    "    dev, L = CFG[\"device\"], CFG[\"lookback\"]\n",
    "    ds = SeriesDataset(arr_std, L, CFG[\"tau\"])\n",
    "    dl = DataLoader(ds, batch_size=CFG[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "\n",
    "    enc, con, head = (\n",
    "        Encoder(L, CFG[\"d_model\"], CFG[\"nhead\"], CFG[\"num_layers\"], CFG[\"dropout\"]).to(dev),\n",
    "        ConceptLayer(CFG[\"d_model\"]).to(dev),\n",
    "        PINNHead().to(dev),\n",
    "    )\n",
    "    opt = torch.optim.AdamW(\n",
    "        list(enc.parameters()) + list(con.parameters()) + list(head.parameters()), lr=CFG[\"lr\"]\n",
    "    )\n",
    "\n",
    "    for ep in trange(CFG[\"epochs\"], desc=\"epochs\", leave=False):\n",
    "        mse_sum, n_sum = 0.0, 0\n",
    "        for win, y in dl:\n",
    "            win, y = win.to(dev), y.to(dev)\n",
    "            C = con(enc(win))\n",
    "            y_hat = head(C)\n",
    "            data = F.mse_loss(y_hat, y)\n",
    "\n",
    "            phys = physics_residual(win[:, -1], C, head)\n",
    "            con_loss = F.mse_loss(C, hard_concepts(win, CFG[\"tau\"]).to(dev))\n",
    "            trend_true, trend_pred = y - win[:, -1], y_hat - win[:, -1]\n",
    "            t_loss = F.mse_loss(trend_pred, trend_true)\n",
    "\n",
    "            loss = data + phys + CFG[\"lamb_con\"] * con_loss + CFG[\"lamb_trend\"] * t_loss\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            mse_sum += (y_hat - y).pow(2).sum().item(); n_sum += y.numel()\n",
    "        if CFG[\"show_epoch_mse\"]:\n",
    "            print(f\"Epoch {ep+1:3d}/{CFG['epochs']}: Train MSE (norm) = {mse_sum / n_sum:.6f}\")\n",
    "    return enc, con, head\n",
    "\n",
    "\n",
    "\n",
    "def predict_last96(arr_std, enc, con, head, scaler):\n",
    "    \"\"\"返回 (raw_true, raw_pred, norm_true, norm_pred) 四元组\"\"\"\n",
    "    dev, L, P = CFG[\"device\"], CFG[\"lookback\"], CFG[\"pred_len\"]\n",
    "    wins = window_stack(arr_std, L)[-P:]\n",
    "    with torch.no_grad():\n",
    "        y_pred_norm = (\n",
    "            head(con(enc(torch.from_numpy(wins).float().to(dev)))).cpu().numpy().ravel()\n",
    "        )\n",
    "    y_true_norm = arr_std[-P:]\n",
    "\n",
    "    \n",
    "    y_pred_raw = scaler.inverse_transform(y_pred_norm.reshape(-1, 1)).ravel()\n",
    "    y_true_raw = scaler.inverse_transform(y_true_norm.reshape(-1, 1)).ravel()\n",
    "    return y_true_raw, y_pred_raw, y_true_norm, y_pred_norm\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = load_csv(CFG[\"csv_path\"], CFG[\"date_col\"])\n",
    "\n",
    "    test_mse_list, test_mae_list = [], []\n",
    "\n",
    "    for col in df.columns:\n",
    "        arr = df[col].dropna().values.astype(\"float32\")\n",
    "        if len(arr) < CFG[\"lookback\"] + CFG[\"pred_len\"] + 10:\n",
    "            continue\n",
    "        scaler = get_scaler(); arr_norm = scaler.fit_transform(arr.reshape(-1, 1)).ravel()\n",
    "\n",
    "        try:\n",
    "            enc, con, head = train_series(arr_norm)\n",
    "            y_true_raw, y_pred_raw, y_true_norm, y_pred_norm = predict_last96(\n",
    "                arr_norm, enc, con, head, scaler\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            warnings.warn(f\"{col}: {e}\"); continue\n",
    "\n",
    "       \n",
    "        mse = float(np.mean((y_pred_norm - y_true_norm) ** 2))\n",
    "        mae = float(np.mean(np.abs(y_pred_norm - y_true_norm)))\n",
    "        test_mse_list.append(mse); test_mae_list.append(mae)\n",
    "        print(f\"{col}: Test MSE (norm) = {mse:.6f}, MAE (norm) = {mae:.6f}\")\n",
    "\n",
    "        \n",
    "        if CFG[\"plot_each\"]:\n",
    "            plt.figure(figsize=(8, 3))\n",
    "            plt.plot(y_true_raw, label=\"True\")\n",
    "            plt.plot(y_pred_raw, label=\"Pred\")\n",
    "            plt.title(f\"{col} – Last {CFG['pred_len']} steps\")\n",
    "            plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    \n",
    "    if test_mse_list:\n",
    "        print(\"\\n----- Overall Normalised Test Results -----\")\n",
    "        print(f\"Series processed : {len(test_mse_list)}\")\n",
    "        print(f\"Average MSE (norm): {np.mean(test_mse_list):.6f}\")\n",
    "        print(f\"Average MAE (norm): {np.mean(test_mae_list):.6f}\")\n",
    "    else:\n",
    "        print(\"No valid series processed – please check the CSV and CFG settings.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-GPU (960M)",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
